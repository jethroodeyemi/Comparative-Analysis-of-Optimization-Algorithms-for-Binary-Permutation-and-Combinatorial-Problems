\section{INTRODUCTION}
Optimization problems are central to decision-making processes in a wide array of fields such as operations research \cite{Padamwar2019},  machine learning \cite{Abdulkadirov2023}\cite{GAMBELLA2021807}, economics \cite{PONSTEIN1984255}, and logistics \cite{GARCIA2015153}. The goal in optimization is to determine the values of decision variables that either maximize or minimize an objective function, often while satisfying a set of constraints. These problems arise in numerous practical applications: from designing efficient supply chains to training machine learning models, managing investment portfolios, or optimizing transportation routes. In machine learning, optimization is critical during the model training process, where the objective is typically to minimize a loss function \cite{Bian2024} that measures the error between predicted and true values. The process of tuning model parameters is often accomplished using gradient-based methods such as gradient descent \cite{Baldi1995} and its variants (e.g., stochastic gradient descent \cite{Tian2023}, Adam \cite{Kingma2014AdamAM}). These methods are highly efficient for convex problems, but they face significant challenges in more complex scenarios, particularly when the objective function \cite{Matsuoka2019} is non-convex, riddled with local minima, or spans a high-dimensional space. To overcome these limitations, random optimization algorithms which uses randomness to explore the solution space more thoroughly have emerged as powerful alternatives. These methods. Random optimization techniques are particularly effective for solving binary \cite{Becerra-Rozas2023}, permutation \cite{Hu2003}, and combinatorial problems \cite{Peres2021}, where traditional gradient-based methods may not be applicable or efficient. 

Optimization problems can vary greatly in their structure, from continuous to discrete, convex to non-convex, and low-dimensional to high-dimensional. Each problem type presents unique challenges, such as local minima, large search spaces, or complex constraints, making it essential to choose the right optimization algorithm. Without a clear understanding of how algorithms behave across different problem domains, there is a risk of selecting inefficient methods that may get stuck in suboptimal solutions, fail to converge, or require excessive computational resources. Therefore, systematically evaluating and comparing algorithms across a diverse set of problem types—such as binary, permutation, and combinatorial problems—is necessary to gain insights into their strengths, limitations, and overall performance.

The objective of this study is to analyze and compare the performance of various optimization algorithms on a suite of benchmark fitness functions. These benchmark functions represent different types of optimization challenges, including binary, permutation, and combinatorial problems, allowing for a comprehensive evaluation of each algorithm's strengths and weaknesses. By systematically testing algorithms across these functions, the study aims to assess key performance metrics such as convergence speed, solution quality, robustness, and computational efficiency. Additionally, we introduce a new combinatorial problem in this study that other researchers can use to further evaluate optimization algorithms in future studies.

The rest of the paper is organized as follows. In Section 2, ….
