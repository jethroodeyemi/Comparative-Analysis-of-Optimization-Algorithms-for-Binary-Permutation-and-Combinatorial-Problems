\section{RELATED WORKS}
Several studies have explored the performance of random optimization algorithms across diverse problem types. For example, Derya (2020) evaluated SGD, AdaGrad, AdaDelta, Adam and RMSProp algorithms in deep learning, focusing on how these algorithms differ in their working principles, strengths, and limitations \cite{Soydaner2020}. There were notable distinctions in their ability to handle noisy gradients, with Adam and RMSProp excelling in adapting learning rates, making them particularly suited for problems involving sparse data and non-convex loss landscapes. In another study, the performance of Genetic Algorithms (GA) and Simulated Annealing (SA) in maximizing the thermal conductance of harmonic lattices, highlighting the strengths of GA in global exploration, while noting SA’s compute efficiency \cite{KERR201931}. Similarly, a comparative analysis of Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO) for distance optimization is presented \cite{GUPTA2020245}.

In this study, we analyze optimization challenges within the categories of binary, permutation, and combinatorial problems. Focusing on permutation problems, a recent investigation explored evolutionary diversity optimization applied to the Traveling Salesperson Problem (TSP) and Quadratic Assignment Problem (QAP) \cite{Do_2022}. The study demonstrated that mutation operators for these permutation problems can ensure convergence towards maximally diverse populations, provided the population size is sufficiently small. Additionally, a Binary version of Equilibrium Optimization (BEO) has been proposed for tackling the 0–1 Knapsack Problem, a classic example of a discrete optimization problem \cite{ABDELBASSET2021106946}. Another study introduced the Global Neighborhood Algorithm (GNA), which balances global and local search in optimization \cite{Alazzam2013}.
